---
title: "Class09 Mini Project"
author: "Michelle Janossy"
date: "4/30/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Breast Cancer data

```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
```

```{r}
wisc.data <- as.matrix(wisc.df[,3:32])
```

```{r}
row.names(wisc.data) <- wisc.df$id
head(wisc.data)
```


Finally, setup a separate new vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.

```{r}
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```



Q1. How many patients are in this dataset?

```{r}
nrow(wisc.data)
```

Q2. How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean", colnames(wisc.data)))
#colnames(wisc.data)
```

Q3. How many of the observations have a malignant diagnosis?

```{r}
sum(diagnosis)
```

```{r}
# Check column means and standard deviations
round( colMeans(wisc.data), 1)

round(apply(wisc.data,2,sd), 1)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis + 1)
```




Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

```{r}
summary(wisc.pr)
```



Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 components 

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 components 

```{r}
biplot(wisc.pr)
```

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

messy and dificult to understand

```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[,1], wisc.pr$x[,2] , col =diagnosis + 1, 
     xlab = "PC1", ylab = "PC2")
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```

Variance Explained

```{r}
# Calculate variance of each component

pr.var <- (wisc.pr$sdev^2)
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- (pr.var/sum(pr.var))*100

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
# Plot cumulative proportion of variance explained
plot(pve, xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
```

```{r}
# Plot cumulative proportion of variance explained
plot( cumsum(pve) , xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
```


Use the par() function to create a side by side plot (i.e. 1 row 2 column arrangement) of these two graphs.



```{r}
## ggplot based graph
#install.packages("factoextra")
#library(factoextra)
```

```{r}
#fviz_eig(wisc.pr, addlabels = TRUE)
```

Hierarchical Clustering

```{r}
# Scale the wisc.data data: data.scaled
#data.scaled <- scale(wisc.data)
```

```{r}
# calculate euclidean distances and assign to data.dist
#data.dist <- dist(data.scaled)
```

```{r}
#Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

#wisc.hclust <- hclust(data.dist, method = "complete")
```

```{r}
# Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

#plot(wisc.hclust)

#abline(h = 19, col="red", lty=2)
```

```{r}
#Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

#wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
#We can use the table() function to compare the cluster membership to the actual diagnoses.

#table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?

Combining Methods

```{r}
data.dist.pr <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(data.dist.pr, method="ward.D2")
wisc.pr.hclust
```

```{r}
plot(wisc.pr.hclust)

```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=diagnosis+1)
```

Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16)
```

Q17. Which of these new patients should we prioritize for follow up based on your results?

the patient in the malignant cluster




